---
title: "Juan Reyes, Ilya Schneider, Francesco de Luca"
format: html
editor: source
self-contained: true
editor_options: 
  chunk_output_type: console
---

## Introduction
U.S. poverty rates was 12.8% in 2021, almost a 2% increase from the pre-pandemic poverty levels. The increases in the national poverty rate over the last two years reversed a trend of five consecutive annual declines. Having the ability to predict if an individual in the US is likely to be in poverty can be incredibly useful to inform public policy-making. By understanding which characteristics are likely to lead to these outcomes, policymakers may be able to more effectively implement targeted poverty reduction polices and once again reverse this trend.

## Motivation for this project
Our project will generate a model to predict whether an individual is likely to be in poverty using 2018 data. We begin with exploratory data analysis to better understand our data and visualize some our findings below. This includes visualizing the difference between two poverty variables in our data set, a bar graph of poverty rates by highest education earned, as well as a line graph of poverty rates by age and race. We generate models.. test it.. work?

## Data sources
For this analysis, we use the Annual Social and Economic Supplemental (ASEC) of the Current Population Survey (CPS).  We accessed this data through the Integrated Public Use Microdata Series (IPUMS) at the University of Minnesota. Our project uses the March Supplemental survey results from 2018. We chose not to use data from before the COVID-19 pandemic to isolate the factors not related to the unforeseen global health crisis.

The projectâ€™s public GitHub repository can be seen [here](https://github.com/ilyaschneider/670_final_project).

Next we load the necessary libraries and read in the data:

``` {r}
library(tidyverse)
library(dplyr)
library(ipumsr)
library(janitor)
library(srvyr)
library(parsnip)
library(survey)
library(tidymodels)
library(patchwork)
library(rpart.plot)
library(lubridate)
library(themis)
library(tune)
library(tigris)
library(randomForest)
library(ranger)
library(glmnet)
```

``` {r}
setwd("G:/MPP/Fall 2022/PPOL 670/Final_project_materials/data")
```

``` {r}
ddi <- read_ipums_ddi("cps_00010.xml")
data <- read_ipums_micro(ddi)
data <- clean_names(data)
```

First we drop extraneous variables, remove multi-state FIPS codes, and recode missing or unavailable data into NAs. We also recode categorical variables into dummy variables and create a dummy variable for immigration status, coded as 1 for people born in the U.S. and 0 for people born in U.S. outlying territories, naturalized citizens, non-citizens, and people born abroad to American parents. We then create the weighted data frame that we will use throughout our analysis.


```{r}
cps_svy <- data %>%
  
  #Remove unnecessary variables
  select(-asecflag, -serial, -month, -cpsid, -cpsidp, -pernum, -asecwth, -himcaidly) %>%
  
  #Remove multi-state FIPS codes
  filter(statefip <= 56) %>%
  
  #Recode missing or otherwise unavailabe data into NAs
  mutate_at(vars(statefip, offpov), list(~ case_when(. == 99 ~ NA_real_, TRUE ~ as.numeric(.)))) %>%
  mutate_at(vars(county, pubhous, ownershp, empstat, labforce, diffany, workly, poverty, himcarely, phinsur, kidcneed, union, yrimmig, foodstmp, stampval, occ, ind, occly, indly, nchlt5, nchild, famsize, age, stampmo), list(~ case_when( . == 0 ~ NA_real_, TRUE ~ as.numeric(.)))) %>%
  mutate_at(vars(metro, migrate1, vetstat), list(~ case_when(. == 0 | . == 9 ~ NA_real_, TRUE ~ as.numeric(.)))) %>%
  mutate(hhincome = case_when(hhincome == 99999999 ~ NA_real_, TRUE ~ as.numeric(hhincome))) %>%
  mutate_at(vars(citizen, marst, sex), list(~ case_when(. == 9 ~ NA_real_, TRUE ~ as.numeric(.)))) %>%
  mutate_at(vars(classwkr, classwly), list(~ case_when(. == 0 | . == 99 ~ NA_real_, TRUE ~ as.numeric(.)))) %>%
  mutate(hispan = case_when(hispan == 901 | hispan == 902 ~ NA_real_, TRUE ~ as.numeric(hispan))) %>%
  mutate(uhrsworkt = case_when(uhrsworkt == 997 | uhrsworkt == 999 ~ NA_real_, TRUE ~ as.numeric(uhrsworkt))) %>%
  mutate(educ = case_when(educ == 001 | educ == 999 ~ NA_real_, TRUE ~ as.numeric(educ))) %>%
  mutate(incss = case_when(incss == 999999 ~ NA_real_, TRUE ~ as.numeric(incss))) %>%
  mutate(incfarm = case_when(incfarm == 99999999 | incfarm == 99999998 ~ NA_real_, TRUE ~ as.numeric(incfarm))) %>%
  mutate(incint = case_when(incint == 9999999 ~ NA_real_, TRUE ~ as.numeric(incint))) %>%
  mutate(bpl = case_when(bpl == 96000 | bpl == 99999 ~ NA_real_, TRUE ~ as.numeric(bpl))) %>%
  mutate(race = case_when(race == 999 ~ NA_real_, TRUE ~ as.numeric(race))) %>%
  
  #Create dummy variable for immigration status:
  mutate(immigrant = case_when(citizen == 1 ~ 0, citizen != 1 ~ 1, TRUE ~ NA_real_)) %>%
  
  #Recode variables as dummies:
  mutate_at(vars(pubhous, foodstmp, sex, vetstat, labforce, diffany, himcarely, phinsur, anycovnw, kidcneed, workly), list(~ case_when(
    . == 2 ~ 1,
    . == 1 ~ 0,
    TRUE ~ NA_real_))) %>%
  
  #Recode poverty in reverse, since we want 1 to be those in poverty:
  mutate_at(vars(offpov), list(~ case_when(
    . == 01 ~ 1,
    . == 02 ~ 0,
    TRUE ~ NA_real_)))

#Weight the data as a survey using individual weights
weighted_cps <- as_survey_design(cps_svy, weights = asecwt)

#Create tibbles by year to use in model construction
weighted_cps_2018 <- as.data.frame(weighted_cps) %>%
  filter(year == 2018)
weighted_cps_2019 <- as.data.frame(weighted_cps) %>%
  filter(year == 2019)
```

IPUMS uses two different measures of poverty, namely `offpov` and `poverty`. The former is used to calculate official poverty rates, correcting for errors in the `poverty` variable by including those only in the poverty universe. The documentation explains that "If one restricts the poverty universe correctly, POVERTY will replicate official poverty rates for most years except for survey years 2000, 1993, 1987-1975 and 1970." Is there considerable difference in the data between how these two variables measure poverty rates between the years? 


```{r}
#Creating bar charts to see the difference between "offpov" and "poverty" in the data 

offpov_bar <- weighted_cps %>%
  drop_na(offpov) %>%
  select(year, offpov) %>%
  group_by(year, offpov) %>%
  summarize(prop = survey_prop()*100) %>%
  ggplot(aes(x = factor(year), y = prop, fill = factor(offpov))) +
  geom_col(position = "dodge") +
  geom_text(aes(label = round(prop, 2)), 
            position = position_dodge(width = 1),
            color="black", vjust = -.2, hjust = .5) +
  labs(
    title = "Using offpov",
    subtitle = "The percentage of people in poverty (offpov) decreased \n from 12.31% to 11.78% between 2018 and 2019.",
    caption = "Source: IPUMS",
    x = "Year",
    y = "Percentage of population",
    fill = "Status"
  ) +
  scale_fill_hue(labels = c("Above poverty line", "Below poverty line")) +
  theme(legend.title = element_blank()) +
  theme_minimal()


poverty_bar_clean <- weighted_cps %>%
  drop_na(poverty) %>%
  #Make poverty a binary indicator
  mutate(poverty = case_when(poverty == 10 ~ 1, poverty >= 20 ~ 0)) %>%
  select(year, poverty) %>%
  group_by(year, poverty) %>%
  summarize(prop = survey_prop()*100) %>%
  ggplot(aes(x = factor(year), y = prop, fill = factor(poverty))) +
  geom_col(position = "dodge") +
  geom_text(aes(label = round(prop, 2)), 
            position = position_dodge(width = 1),
            color="black", vjust = -.2, hjust = .5) +
  labs(
    title = "Using poverty",
    subtitle = "The percentage of people in poverty (poverty) decreased \n from 12.47% to 11.92% between 2018 and 2019.",
    caption = "Source: IPUMS",
    x = "Year",
    y = "Percentage of population",
    fill = "Status"
  ) +
  scale_fill_hue(labels = c("Above poverty line", "Below poverty line")) +
  theme(legend.title = element_blank()) +
  theme_minimal()
  

poverty_bar_detail <- weighted_cps %>%
  drop_na(poverty) %>% 
  #Reverse the categories
  mutate(poverty = case_when(
    poverty == 10 ~ 3,
    poverty == 21 ~ 2,
    poverty == 22 ~ 1,
    poverty == 23 ~ 0
  )) %>%
  select(year, poverty) %>%
  group_by(year, poverty) %>%
  summarize(prop = survey_prop()*100) %>%
  ggplot(aes(x = factor(year), y = prop, fill = factor(poverty))) +
  geom_col(position = "dodge") +
  geom_text(aes(label = round(prop, 2)), 
            position = position_dodge(width = 1),
            color="black", vjust = -.2) +
  labs(
    title = "Percentage of Population Relative to the Poverty Line, using poverty",
    subtitle = "The percentage of people in poverty (poverty) and close to poverty decreased between 2018 and 2019. \n Approximately 8 to 8.5% of the population were close to poverty in those years.",
    caption = "Source: IPUMS",
    x = "Year",
    y = "Percentage of population",
    fill = "Status"
  ) +
  scale_fill_hue(labels = c("150 percent and above the low-income level", "125-149 percent of the low-income level", "100-124 percent of the low-income level", "Below poverty line")) +
  theme(legend.title = element_blank()) +
  theme_minimal()


((offpov_bar + poverty_bar_clean ) / poverty_bar_detail) + plot_annotation(title = 'Percentage of Population in Poverty')
```

As the bar-charts above show, there is a very minor difference in poverty rates between `offpov` and `poverty`. In both 2018 and 2019, the difference in poverty rate was less than 0.25 percentage points in magnitude between the two variables. We can conclude that these two variables are very similar. That being said, the poverty levels remained relatively constant between 2018 and 2019, as no major disruptions in the economic markets were occurring at the time. Since no major differences are visible, we will use `offpov` as our primary outcome variable.


```{r}
bar_by_educ <- weighted_cps %>%
  drop_na(offpov, educ) %>%
  filter(age >= 18) %>%
  mutate(max_educ = case_when(
    educ <= 71 ~ 0, #Less than HS diploma
    educ == 73 ~ 1, #HS diploma
    educ <= 92 & educ > 73 ~ 2, #Some college/Associate's
    educ == 111 ~ 3, #Bachelor's
    educ > 111 ~ 4 #Master's, professional, doctorate
  )) %>%
  select(offpov, max_educ) %>%  
  group_by(max_educ) %>%
  srvyr::summarize(pov = survey_mean(offpov)*100) %>%
  ggplot(aes(x = max_educ, y = pov, fill = factor(max_educ))) +
  geom_col() +
  geom_text(aes(label = round(pov, 2)), vjust = -.5) +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  scale_x_continuous(breaks = NULL) +
  labs(
    title = "Percentage of Adults (above 18) in Poverty by Highest Education Earned",
    subtitle = "Those without a HS degree experience the most poverty, at almost 25%. \nHaving a graduate degree lowers your chances of poverty to 3.44%.",
    caption = "Source: IPUMS",
    x = "Highest level of education", 
    y = "Percentage of group in poverty", 
    fill = "Highest Education Earned"
  ) +
  scale_fill_hue(labels = c("Less than HS diploma", "HS diploma", "Some college/Associate's degree", "Bachelor's degree", "Master's/Professional/Doctorate degree"))
  
bar_by_educ
```

This bar graph shows that poverty rates in 2018 decreased by level of education earned. In particular those who earned less than a high school diploma had the highest incidence of poverty, while those who earned higher than a bachelorâ€™s degree had the lowest. It is notable that the most significant difference in the incidence of poverty occurred between those who earned less than a high school diploma, and those who earned a HS diploma: an 11% difference between these two groups. It appears that education can be a powerful predictor of likelihood of poverty for an individual, and increasing high school graduation rates might be beneficial in reducing poverty rates.


```{r}
line_by_age <- weighted_cps %>%
  drop_na(race, offpov, age) %>%
  select(race, offpov, age) %>%
  mutate(Race = factor(case_when(
    race == 100 ~ "White",
    race != 100 ~ "Non-White"
  ))) %>%
  #filter(age < 80) %>% Since anyone between 80 and 84 is coded as 80, and anyone 85 or above is coded as 85
  group_by(Race, age) %>%
  srvyr::summarize(pov = survey_mean(offpov)*100) %>%
  ggplot(aes(x = age, y = pov, color = Race)) +
  geom_line(linewidth = 1) +
  scale_x_continuous(expand = expansion(mult = c(0.002, 0))) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.002))) +
  labs(
    title = "Poverty Rate by Age and Race",
    subtitle = "Whites experience much lower poverty rates than non-Whites at all ages. \nPoverty drops when people enter working age, increases around 50, and drops again at retirement.",
    caption = "Source: IPUMS",
    x = "Age", 
    y = "Percentage of population in poverty",
    fill = "Race"
  ) +
  theme_minimal()

line_by_age
```

Measuring poverty rate in 2018 by age and race it is evident that whites have a significantly lower incidence of poverty than non-whites. The disparity in poverty rates between whites and non-whites is most significant at younger ages, and falls as individuals enter the workforce. Interestingly, we can see that non-whites incidence of poverty increases again around age 45 and that this gap in poverty rates widen as they age. Poverty rates decline briefly after retirement, which can likely be attributed to people getting access to their social security benefits and government provided healthcare, but they rise again as people enter their 70's and 80's. 


```{r}
states <- tigris::states(cb = TRUE, year = 2018) %>%
  filter(!STATEFP %in% c("60", "69", "72", "78", "66", "15", "02"))

#Group by state and calculate the poverty rate
cps_svy_map <- as_survey_design(cps_svy, weights = asecwt) %>%
  filter(year == 2018, !is.na(offpov)) %>%
  group_by(statefip) %>%
  srvyr::summarize(pov_rate = survey_mean(offpov)) %>%
  rename(STATEFP = statefip)

cps_svy_map$STATEFP <- as.character(cps_svy_map$STATEFP)
cps_svy_map$STATEFP <- str_pad(cps_svy_map$STATEFP, 2, c("left"), "0")
  
joined_data <- left_join(states, cps_svy_map, by = "STATEFP")

map <- cps_svy_map %>%
  ggplot() +
  geom_sf(joined_data, mapping = aes(fill = as.numeric(pov_rate))) +
  scale_fill_gradient(name = "Poverty Rate", low = "lightblue", high = "darkblue") +
  theme_void() +
  labs(
    title = "Poverty Rate in the Contiguous U.S. in 2018",
    subtitle= "State-level poverty rates range from 6.6% in New Hampshire to 21% in Louisiana."
  )

map
```

Looking at the map of contiguous U.S. we can observe there is a wide variation of poverty rates by state. There appears to be a a greater concentration of poverty in southern states such as Louisiana, Arizona, and Mississippi. Meanwhile the lowest incidence of poverty can be seen in the Northeast, along with Colorado and Utah. This map demonstrates that although there are clearly states with a greater concentration of poverty, state geography is likely not the greatest predictor to determine whether an individual will be in poverty.

## Modeling

First, we make a basic decision tree to understand which variables were the most useful in predicting poverty in our data frame. We excluded from our model the other measure of poverty (`poverty`), as well as household income, from the model, since both of those are directly predictive of poverty, and are thus not useful to us.

```{r}
#Creating data frame for the analysis
weighted_cps <- as_survey_design(cps_svy, weights = asecwt) %>%
  select(-poverty, -hhincome)
weighted_cps <- as.data.frame(weighted_cps)

weighted_cps_models <- weighted_cps %>%
  drop_na(offpov) %>%
  mutate(offpov = as.factor(offpov)) %>% 
  select(-year) %>%
  mutate_at(vars(immigrant,
                 pubhous,
                 foodstmp,
                 sex,
                 vetstat,
                 labforce,
                 diffany,
                 himcarely,
                 phinsur,
                 anycovnw,
                 kidcneed,
                 workly), 
            list(~ as.factor(.))
  )

set.seed(20211101)

#Creating a split object:
weighted_cps_split <- initial_split(data = weighted_cps_models, prop = 0.8)

#Creating the training and testing data:
weighted_cps_train <- training(x = weighted_cps_split) 
weighted_cps_test <- testing(x = weighted_cps_split)

#Creating a recipe:
cart_rec <-
  recipe(formula = offpov ~ ., data = weighted_cps_train)

#Creating a cart model object:
cart_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")
cart_wf <- workflow() %>%
  add_recipe(cart_rec) %>%
  add_model(cart_mod)

#Fitting the model:
cart_fit <- cart_wf %>%
  fit(data = weighted_cps_train)

# Creating a tree:
rpart.plot::rpart.plot(x = cart_fit$fit$fit$fit, roundint=FALSE)

# Predicting the predicted class and the predicted probability of each class:
predictions <- bind_cols(
  weighted_cps_test,
  predict(object = cart_fit, new_data = weighted_cps_test),
  predict(object = cart_fit, new_data = weighted_cps_test, type = "prob")
)
select(predictions, offpov, starts_with(".pred"))
```

As we can see in this decision tree, important variables in predicting whether someone was in poverty or not are: whether they received food stamps (`foodstmp`, currently known as SNAP), and if so, how much (`stampval`); total family medical out of pocket payments (`moop`); whether the respondent worked last year (`workly`); their Social Security income (`incss`); and family size (`famsize`). Those who received SNAP benefits and had high out of pocket medical expenditures were more likely to end up in poverty.


The next step is to construct a random forest:

First we create the data frame we will use for our modeling, filtering out NA values from offpov, and turning variables into factors. We then set the seed and split the data into a test set and a training set. Next we created a recipe for the random forest model, turning categorical variables into dummy variables. Lastly, we created a random forest model setting the engine to ranger and mode to classification. After creating a workflow, we create a grid for parameters and then attempt to execute it. At this point, our model fails to run any further. Unfortunately, we were unable to calculate the ROC_AUC or accuracy of our model for this reason. If successful, the random forest model would have allowed us to isolate the variables of greatest importance for predicting poverty at an individual level.

```{r}
# Creating dataframe for random forest model
rf_model_cps_2018 <- weighted_cps_2018 %>%
  filter(!is.na(offpov)) %>%
 mutate(offpov = as.factor(offpov)) %>% # Here we make our y variable a factor variable
 select (- year  , - county , -poverty , -hhincome , -asecwt , -stampval , yrimmig , moop) %>% # We remove variables we don't want to include as predictors
   mutate_at(vars(metro, statefip, pubhous, vetstat, empstat, labforce, occ, ind, classwkr, uhrsworkt, educ, diffany, occly, indly, classwly, workly, incint, migrate1, himcarely, kidcneed, union), list(~ as.factor(.))) 

```

```{r}
# Set seed so model is consistent
set.seed(12122022)
# Split data frame into training and testing data
split <- initial_split(rf_model_cps_2018)

rf_cps_2018_train <- remove_val_labels(training(split))
rf__cps_2018_test <- remove_val_labels(testing(split))

# Set up 5 folds
folds <- vfold_cv(data = rf_cps_2018_train, v = 5)

# Create a recipe for random forest model 

rf_rec <-
  recipe(offpov ~ ., data = rf_cps_2018_train) %>%
  step_dummy(metro, statefip, pubhous, vetstat, empstat, labforce, occ, ind, classwkr, uhrsworkt, educ, diffany, occly, indly, classwly, workly, incint, migrate1, himcarely, kidcneed, union) %>% #Turn select categorical variables into dummies
  step_center(all_numeric_predictors()) %>% # this centers predictors
  step_scale(all_numeric_predictors()) %>% # this scales predictors
  step_nzv(all_numeric_predictors()) # this drops near zero variance predictors
  
```

``` {r}
# Making a random forest model 
rf_model <- rand_forest(mtry = 10, min_n = tune(), trees = 100) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

# Create a workflow
rf_workflow <-
  workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_rec)

# Create a grid of the parameters
rf_grid <- grid_regular(
  min_n(range = c(2, 8)),
  levels = 4)

# Execute hyperparameter tuning using the grid and the cross_validation folds
rf_cv <- tune_grid(rf_workflow,
                   resamples = folds,
                   grid = rf_grid,
                   metrics = metric_set(roc_auc))

# Calculate ROC_AUC and accuracy 
collect_metrics(rf_cv, summarize = TRUE) %>%
  filter(.metric == "roc_auc")

# Select the best model based on roc_auc
rf_best_model <- rf_cv %>%
  select_best(metric = "roc_auc")

# Finalize model
rf_final_model <- finalize_model(rf_model, rf_best_model)
# Evaluate variable importance
rf_last_fit %>%
  extract_fit_parsnip() %>%
  vip::vip(num_features = 15)
rf_predictions <- bind_cols(rf__cps_2018_test,
                            predict(object = rf_last_fit, new_data = rf__cps_2018_test),
                            predict(object = rf_last_fit, new_data = rf__cps_2018_test, type = "prob"))

# To evaluate the model calculate...
# Confusion matrix
conf_mat(data = rf_predictions,
         truth = offpov,
         estimate = .pred_class)
# Accuracy
accuracy(data = rf_predictions,
          truth = offpov,
          estimate = .pred_class)
# ROC_AUC metric
roc_auc(data = rf_predictions,
     truth = employed,
     estimate = .pred_0)
```


The next attempted model was k nearest neighbors: 

In the KNN model, we attempted to predict the value of `offpov` with the predictive variables in the weighted CPS data frame. We decided to tune the number of neighbors based on the best model that would have resulted from the fit into the folds we created. Unfortunately,  we were unable to fit the model because of an error we kept incurring into. The error message appeared whenever we tried to fit the model into the folds we created, and didn't disappear no matter how we tweaked the data or the steps in the model's recipe.


```{r}
#Creating folds:
folds <- vfold_cv(data = weighted_cps_train, v = 5)
#Creating a recipe object:
weighted_cps_rec <- 
  recipe(formula = offpov ~ ., data = weighted_cps_train) %>%
  step_dummy(immigrant, 
             pubhous, 
             foodstmp, 
             sex, 
             vetstat, 
             labforce, 
             diffany, 
             himcaidly, 
             himcarely, 
             phinsur, 
             anycovnw, 
             kidcneed, 
             workly)
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  step_impute_knn(all_predictors()) %>%
  step_other()
  
# Creating a KNN model with hyper parameter tuning:
# Creating a KNN model specification:
weighted_cps_knn_mod <-
  nearest_neighbor(neighbors = tune()) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "classification")
# Creating a workflow:
weighted_cps_knn_workflow <-
  workflow() %>%
  add_model(spec = weighted_cps_knn_mod) %>%
  add_recipe(recipe = weighted_cps_rec)
# Creating a tuning grid:
weighted_cps_knn_grid <- grid_regular(neighbors(range = c(1, 15)), levels = 2)
# Estimating models with re-sampling for each row in the tuning grid:
weighted_cps_knn_res <- weighted_cps_knn_workflow %>% 
  tune_grid(resamples = folds,
            grid = weighted_cps_knn_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(accuracy))
```


Finally, our last intended model was a LASSO model:

The process we went through for this model was similar as for the others. We cleaned the data for this specific model, which included turning all the NAs into 0s. We then went through the process of splitting the data, creating a recipe, making a model, and a workflow, and attempting to apply the new model to our data. Alas, we ran into an issue here as well. This time, however, we managed to get the RMSE associated with each fold. No amount of fiddling with the recipe or cleaning of the data resulted in a successful model.

```{r}
#Preparing the data

#Deleting year
weighted_cps_2018_lasso <- weighted_cps_2018 %>%
  select(-year, -asecwt)

#Turning NAs into 0s
weighted_cps_2018_lasso[is.na(weighted_cps_2018_lasso)] <- 0

set.seed(12122022)
split <- initial_split(weighted_cps_2018_lasso)

weighted_cps_2018_train <- training(split)
weighted_cps_2018_test <- testing(split)

#Set up resampling
folds1 <- vfold_cv(weighted_cps_2018_train, v = 5)

#Create a tuning grid for lasso regularization
lasso_grid <- grid_regular(penalty(), levels = 10)

#Creating a recipe
weighted_cps_2018_rec <- recipe(offpov ~ ., data = weighted_cps_2018_train) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_center(all_predictors()) %>% # center predictors
  step_scale(all_predictors()) %>% # scale predictors
  step_nzv(all_predictors()) # drop near zero variance for all predictors
  #step_other(all_predictors())  # collapse low-frequency categories
 

#See the engineered training data
bake(prep(weighted_cps_2018_rec, training = weighted_cps_2018_train), new_data = weighted_cps_2018_train)


#Create a model
lasso_mod <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

#Create a workflow 
lasso_wf <- workflow() %>%
  add_recipe(weighted_cps_2018_rec) %>%
  add_model(lasso_mod)

#Perform hyperparameter tuning
lasso_cv <- lasso_wf %>%
  fit(weighted_cps_2018_train) %>%
  tune_grid(resamples = folds1,grid = lasso_grid)

collect_metrics(lasso_cv, summarize = FALSE)

  
#Select the best model based on the "rmse" metric
lasso_best <- lasso_cv %>%
  select_best(metric = "rmse")
  
# use the finalize_workflow() function with your lasso workflow and the best model 
# to update (or "finalize") your workflow by modifying the line below
lasso_final <- lasso_wf %>%
  finalize_workflow(parameters = lasso_best)

# fit to the training data and extract coefficients
lasso_coefs <- lasso_final %>%
  fit(data = weighted_cps_2018_train)

lasso_predictions <- bind_cols(weighted_cps_2018_test,
                               predict(object = lasso_final, new_data = weighted_cps_2018_test),
                               predict(object = lasso_final, new_data = weighted_cps_2018_test, type = "prob"))
```

Had this model succeeded, we would have attempted to measure its quality, using accuracy, precision, and other metrics discussed in class.

```{r}
conf_mat(data = lasso_predictions, truth = offpov, estimate = .pred_class)
accuracy(data = lasso_predictions, truth = offpov, estimate = .pred_class)
precision(data = lasso_predictions, truth = offpov, estimate = .pred_class)
spec(data = lasso_predictions, truth = offpov, estimate = .pred_class)
```